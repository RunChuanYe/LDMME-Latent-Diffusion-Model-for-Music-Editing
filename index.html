<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <!-- TODO: open graph -->
    <meta name="description" content="Latent Diffusion Model for Music Editing">
    <!-- <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
    <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
    <meta property="og:url" content="URL OF THE WEBSITE" /> -->
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <!-- <meta property="og:image" content="static/image/your_banner_image.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" /> -->

    <!-- TODO: twitter -->
    <!-- <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
    <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG"> -->
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
    <meta name="twitter:card" content="summary_large_image"> -->

    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="Diffusion Model, Music Editing, Music Generation, Music Demos">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>LDMME: Latent Diffusion Model for Music Editing</title>
    <!-- TODO -->
    <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>

    <style>
        /* #tablesContainer { */
        /* display: flex; */
        /* justify-content: center; 居中对齐 */
        /* align-items: center; 垂直居中对齐 (如果需要) */
        /* height: 100vh; 使容器填满视口高度（可选） */
        /* } */
        table {
            width: 80%;
            /* 或任何固定宽度 */
            border-collapse: collapse;
        }

        td,
        th {
            border: 1px solid #ddd;
            padding: 8px;
        }
    </style>

    <style>
        .center-image {
            display: block;
            margin: 0 auto; /* 左右外边距自动 */
        }
    </style>


</head>

<body>


    <section class="hero">
        <div class="hero-body">
            <!-- <div class="container is-max-desktop"> -->
            <div class="container">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">LDMME: Latent Diffusion Model for Music Editing</h1>
                        <div class="is-size-5 publication-authors">
                            <!-- Paper authors -->
                            <span class="author-block">
                                Runchuan Ye<sup>1, 3</sup>,</span>
                            <span class="author-block">Shiyin Kang<sup>2</sup>,</span>
                            <span class="author-block">Zhiyong Wu<sup>3</sup></span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <sup>1</sup> Harbin Institute of Technology (Shenzhen), China
                                <br><sup>2</sup> Skywork AI PTE. LTD., Beijing, China
                                <br><sup>3</sup> Shenzhen International Graduate School, Tsinghua University, Shenzhen,
                                China
                                <br><a href="https://www.ncmmsc.org.cn/">National Conference on Man-Machine Speech
                                    Communication (NCMMSC2024)</a></span>
                            <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- Arxiv PDF link -->
                                <span class="link-block">
                                    <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>

                                <!-- Supplementary PDF link -->
                                <span class="link-block">
                                    <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Supplementary</span>
                                    </a>
                                </span>

                                <!-- Github link -->
                                <span class="link-block">
                                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>

                                <!-- ArXiv abstract Link -->
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>arXiv</span>
                                    </a>
                                </span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <!-- Teaser video-->
    <!-- <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
    <!-- Your video here -->
    <!-- <source src="static/videos/banner_video.mp4" type="video/mp4">
                </video>
                <h2 class="subtitle has-text-centered">
                    Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at,
                    placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut
                    maximus.
                </h2>
            </div>
        </div>
    </section> -->
    <!-- End teaser video -->

    <!-- Paper abstract -->
    <section class="section hero is-light">
        <!-- <div class="container is-max-desktop"> -->
        <div class="container">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            Diffusion models are widely used in the fields of image, audio,
                            and speech generation. While diffusion models for image editing
                            have been extensively studied, their application to music-editing tasks
                            has been relatively neglected. The current research has several shortcomings:
                            1) lack of support for 44.1KHz stereo music, 2) generation of
                            non-music content during editing, and 3) an editing algorithm that does
                            not fully consider existing information, resulting in unnatural edits. To
                            address these limitations, we propose the LDMME model. We first create
                            a high-quality 44.1KHz sampling rate stereo audio dataset, which
                            excluded non-music data, to train the LDMME. In addition, we enhance
                            the quality of the generated music by strengthening the detailed modeling
                            capability of LDMME. Subsequently, we improve the existing editing
                            algorithm by considering the original music information during editing
                            to enhance the naturalness of the edited music. In both music generation and editing tasks,
                            the LDMME model outperforms AudioLDM and
                            MusicLDM according to various subjective and objective metrics. The
                            samples are available on this website:
                            https://runchuanye.github.io/LDMME-Latent-Diffusion-Model-for-Music-Editing/.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <!-- End paper abstract -->

    <!-- Model Architecture -->
    <section class="hero is-small is-light">
        <div class="hero-body">
            <div class="container">
                <h2 class="title">Model Architecture</h2>

                <!-- <iframe src="static/pdfs/train_infer_v3.pdf" width="100%" height="550">
                </iframe>
                <h2 class="subtitle has-text-centered">
                    Fig. 1. Overview of the proposed LDMME. Modules with a lock on the bottom right indicate that they freeze during training.
                </h2> -->
                <img src="static/pdfs/Model_Architecture.png" class="center-image"/>
                <p>
                The model architecture of the proposed LDMME is shown in Fig 1. We base our
                LDMME implementation on the code of Stable Audio [7]. In particular, we use
                a pre-trained DAC [13] as the audio codec and a pre-trained CLAP [24] Text
                Encoder to encode text conditions. Drawing inspiration from Stable Audio [7],
                the Unet model is developed based on Mousai [21] and integrates Seconds Total
                and Seconds Start encoders for training and inference across music of varying
                lengths. It is important to note that our model processes both input and output
                audio as waveform signals, rather than symbolic signals.
                </p>
                <small><br>[7] Evans, Z., Carr, C., Taylor, J., Hawley, S.H., Pons, J.: Fast timing-conditioned latent
                    audio diffusion. arXiv preprint arXiv:2402.04825 (2024)</small>
                <small><br>[13] Kumar, R., Seetharaman, P., Luebs, A., Kumar, I., Kumar, K.: High-fidelity audio
                    compression with improved rvqgan. Advances in Neural Information Processing Systems 36
                    (2024)</small>
                <small><br>[21] Schneider, F., Kamal, O., Jin, Z., Schölkopf, B.: Mo\ˆ usai: Text-to-music generation
                    with long-context latent diffusion. arXiv preprint arXiv:2301.11757 (2023)</small>
                <small><br>[24] Wu, Y., Chen, K., Zhang, T., Hui, Y., Berg-Kirkpatrick, T., Dubnov, S.: Largescale
                    contrastive language-audio pretraining with feature fusion and keyword-tocaption augmentation. In:
                    ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).
                    pp. 1–5. IEEE (2023)</small>

            </div>
        </div>
    </section>
    <!--End Model Architecture -->


    <section class="hero is-small is-light">
        <div class="hero-body">
            <div class="container">
                <h2 class="title">Music Editing Definition</h2>
                <p>
                    <b>Smooth Concatenation of Music Segments:</b> For the smooth splicing of two music segments
                    task, the model accepts two music segments and subsequently splices them into
                    one segment, taking the splicing of the two segments as the to-be-modified portion,
                    and the other portions remain unchanged. Samples are available at <a href="#table2">Table 2: Smooth
                        Concatenation of Music Segments.</a>
                </p>
                <p>
                    <b>Style Transfer of Music Segments Based on Text Descriptions:</b> To perform stylized modification
                    of a music segment based on textual descriptions,
                    we take an audio piece along with specified start and end times for
                    the segment to be altered. The designated segment is then modified according
                    to the provided text description. The part of the music that does not need to
                    be modified remains unchanged. Samples are available at <a href="#table3">Table 3: Style Transfer of
                        Music Segments Based on Text Descriptions. </a>
                </p>
            </div>
        </div>
    </section>


    <!-- table of content -->
    <section class="section is-small is-light">
        <div class="hero-body">
            <div class="container">
            <div id="toc">
                <h2 class="title is-3 is-center">Table of Contents</h2>
                Here, we show examples from music generation and music editing tasks. Tables 1 and 4 are examples of music generation, while Tables 2 and 3 are examples of music editing.
                <br />
                <br />
                <ul class="has-text-centered">
                    <li><a href="#table1">Table 1: Music Generation Based on Text Descriptions.</a></li>
                    <li><a href="#table2">Table 2: Smooth Concatenation of Music Segments.</a></li>
                    <li><a href="#table3">Table 3: Style Transfer of Music Segments Based on Text Descriptions. </a></li>
                    <li><a href="#table4">Table 4: Music Generation Based on Text Descriptions Generated from GPT. </a></li>
                    <!-- <li><a href="#table5">Table 5: Music Generation Based on Text Descriptions Generated from GPT norm. </a></li> -->
                </ul>
                <small>
                <br />
                Table 1 shows music generation based on text descriptions from the test dataset. 
                Table 4 shows music generation based on text descriptions generated by GPT.
                <br />
                Table 2 illustrates the Smooth Concatenation of Music Segments task, where the content from 2.5 to 7.5
                seconds of "Concatenation" is modified by AudioLDM, MusicLDM, or LDMME (Ours), while the rest remains
                unchanged.
                <br />
                Table 3 illustrates the Style Transfer of Music Segments Based on Text Descriptions task, where the content
                from 2.5 to 7.5 seconds of "GT" is modified by AudioLDM, MusicLDM, or LDMME (Ours) based on text conditions, while the rest remains
                unchanged.
                </small>
            </div>
            </div>
        </div>
    </section>

    <!-- music samples -->
    <section class="section hero">
        <div class="hero-body">
            <div class="container is-center has-text-centered" id="tablesContainer">
            </div>
        </div>
    </section>


    <!-- Image carousel -->
    <!-- <section class="hero is-small">
        <div class="hero-body">
            <div class="container">
                <div id="results-carousel" class="carousel results-carousel">
                    <div class="item"> -->
    <!-- Your image here -->
    <!-- <img src="static/images/carousel1.jpg" alt="MY ALT TEXT" />
                        <h2 class="subtitle has-text-centered">
                            First image description.
                        </h2>
                    </div>
                    <div class="item"> -->
    <!-- Your image here -->
    <!-- <img src="static/images/carousel2.jpg" alt="MY ALT TEXT" />
                        <h2 class="subtitle has-text-centered">
                            Second image description.
                        </h2>
                    </div>
                    <div class="item"> -->
    <!-- Your image here -->
    <!-- <img src="static/images/carousel3.jpg" alt="MY ALT TEXT" />
                        <h2 class="subtitle has-text-centered">
                            Third image description.
                        </h2>
                    </div>
                    <div class="item"> -->
    <!-- Your image here -->
    <!-- <img src="static/images/carousel4.jpg" alt="MY ALT TEXT" />
                        <h2 class="subtitle has-text-centered">
                            Fourth image description.
                        </h2>
                    </div>
                </div>
            </div>
        </div>
    </section> -->
    <!-- End image carousel -->


    <!-- Youtube video -->
    <!-- <section class="hero is-small is-light">
        <div class="hero-body">
            <div class="container"> -->
    <!-- Paper video. -->
    <!-- <h2 class="title is-3">Video Presentation</h2>
                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">

                        <div class="publication-video"> -->
    <!-- Youtube embed code here -->
    <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0"
                                allow="autoplay; encrypted-media" allowfullscreen></iframe>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section> -->
    <!-- End youtube video -->


    <!-- Video carousel -->
    <!-- <section class="hero is-small">
        <div class="hero-body">
            <div class="container">
                <h2 class="title is-3">Another Carousel</h2>
                <div id="results-carousel" class="carousel results-carousel">
                    <div class="item item-video1">
                        <video poster="" id="video1" autoplay controls muted loop height="100%"> -->
    <!-- Your video file here -->
    <!-- <source src="static/videos/carousel1.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-video2">
                        <video poster="" id="video2" autoplay controls muted loop height="100%"> -->
    <!-- Your video file here -->
    <!-- <source src="static/videos/carousel2.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-video3">
                        <video poster="" id="video3" autoplay controls muted loop height="100%">\ -->
    <!-- Your video file here -->
    <!-- <source src="static/videos/carousel3.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
            </div>
        </div>
    </section> -->
    <!-- End video carousel -->


    <!-- Paper poster -->
    <!-- <section class="hero is-small is-light">
        <div class="hero-body">
            <div class="container">
                <h2 class="title">Poster</h2>

                <iframe src="static/pdfs/sample.pdf" width="100%" height="550">
                </iframe>

            </div>
        </div>
    </section> -->
    <!--End paper poster -->


    <!--BibTex citation -->
    <!-- <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>BibTex Code Here</code></pre>
        </div>
    </section> -->
    <!--End BibTex citation -->


    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">

                        <p>
                            This page was built using the <a
                                href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
                            You are free to borrow the of this website, we just ask that you link back to this page in
                            the footer. <br> This website is licensed under a <a rel="license"
                                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                                Commons Attribution-ShareAlike 4.0 International License</a>.
                        </p>

                    </div>
                </div>
            </div>
        </div>
    </footer>

    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

    <script>
        /**
         * Load descriptions from a text file.
         * @param {string} filePath - Path to the text file with descriptions.
         * @returns {Promise<Array>} - Promise resolving to an array of descriptions.
         */
        async function loadDescriptions(filePath) {
            try {
                const response = await fetch(filePath);
                const text = await response.text();
                return text.split('\n').filter(line => line.trim() !== '');
            } catch (error) {
                console.error('Error loading descriptions:', error);
                return [];
            }
        }

        /**
         * Create and populate a new table.
         * @param {string} containerId - ID of the container to append the table to.
         * @param {Array} descriptions - Array of descriptions.
         * @param {Array} paths - Array of audio file paths.
         * @param {string} tableTitle - Title of the table.
         * @param {Array} headers - Array of table headers.
         */
        function createTable(containerId, descriptions, paths, tableTitle, headers, tableId) {
            const container = document.getElementById(containerId);

            // Create table elements
            const table = document.createElement('table');
            const tableTitleElement = document.createElement('h2');
            tableTitleElement.textContent = tableTitle;
            tableTitleElement.className = "is-center title is-2 publication-title"
            tableTitleElement.id = tableId
            container.appendChild(tableTitleElement);

            // Create table header
            const thead = document.createElement('thead');
            const headerRow = document.createElement('tr');
            headers.forEach(header => {
                const th = document.createElement('th');
                th.textContent = header;
                headerRow.appendChild(th);
            });
            thead.appendChild(headerRow);
            table.appendChild(thead);

            // Create table body
            const tbody = document.createElement('tbody');
            descriptions.forEach((description, index) => {
                // Description row
                const descriptionRow = document.createElement('tr');
                const numberCell = document.createElement('td');
                numberCell.textContent = index + 1;
                // numberCell.textContent = '';
                descriptionRow.appendChild(numberCell);

                const descriptionCell = document.createElement('td');
                descriptionCell.textContent = description;
                descriptionCell.className = "is-2 bold"
                descriptionCell.colSpan = headers.length - 1; // Span across all audio columns
                descriptionRow.appendChild(descriptionCell);

                // Audio row
                const audioRow = document.createElement('tr');
                const numberCellAudio = document.createElement('td');
                // numberCellAudio.textContent = index + 1;
                numberCellAudio.textContent = '';
                audioRow.appendChild(numberCellAudio);

                paths.forEach(path => {
                    const audioCell = document.createElement('td');
                    const audio = document.createElement('audio');
                    audio.controls = true;
                    const source = document.createElement('source');
                    source.src = `${path}/${index + 1}.mp3`;
                    source.type = 'audio/mpeg';
                    audio.appendChild(source);
                    audioCell.appendChild(audio);
                    audioRow.appendChild(audioCell);
                });

                // Separator row
                const separatorRow = document.createElement('tr');
                const separatorCell = document.createElement('td');
                separatorCell.colSpan = headers.length;
                separatorCell.innerHTML = '<br/>'
                // separatorCell.className = 'separator';
                separatorRow.appendChild(separatorCell);

                tbody.appendChild(descriptionRow);
                tbody.appendChild(audioRow);
                tbody.appendChild(separatorRow);
            });
            table.appendChild(tbody);

            // Append the table to the container
            container.appendChild(table);
        }

        // Initialize the tables with specific configurations
        async function initializeTables() {
            const tablesConfig = [
                {
                    descriptionsPath: 'static/samples/gen/descriptions.txt',
                    audioPaths: [
                        // 'static/samples/gen/samples_gt',
                        // 'static/samples/gen/samples_audioldm',
                        // 'static/samples/gen/samples_musicldm',
                        // 'static/samples/gen/samples_stereo'
                        'static/samples/gen/samples_gt_norm',
                        'static/samples/gen/samples_audioldm_norm',
                        'static/samples/gen/samples_musicldm_norm',
                        'static/samples/gen/samples_stereo_norm'
                    ],
                    tableTitle: 'Table 1: Music Generation Based on Text Descriptions.',
                    tableId: "table1",
                    headers: ['#', 'GT', 'AudioLDM', 'MusicLDM', 'LDMME (Ours)']
                },
                {
                    descriptionsPath: 'static/samples/concat/descriptions.txt',
                    audioPaths: [
                        // 'static/samples/concat/samples_gt',
                        // 'static/samples/concat/samples_audioldm',
                        // 'static/samples/concat/samples_musicldm',
                        // 'static/samples/concat/samples_stereo'
                        'static/samples/concat/samples_gt_norm',
                        'static/samples/concat/samples_audioldm_norm',
                        'static/samples/concat/samples_musicldm_norm',
                        'static/samples/concat/samples_stereo_norm'
                    ],
                    tableTitle: 'Table 2: Smooth Concatenation of Music Segments.',
                    tableId: "table2",
                    headers: ['#', 'Concatenation', 'AudioLDM', 'MusicLDM', 'LDMME (Ours)']
                },
                {
                    descriptionsPath: 'static/samples/style_transfer/descriptions.txt',
                    audioPaths: [
                        // 'static/samples/style_transfer/samples_gt',
                        // 'static/samples/style_transfer/samples_audioldm',
                        // 'static/samples/style_transfer/samples_musicldm',
                        // 'static/samples/style_transfer/samples_stereo'
                        'static/samples/style_transfer/samples_gt_norm',
                        'static/samples/style_transfer/samples_audioldm_norm',
                        'static/samples/style_transfer/samples_musicldm_norm',
                        'static/samples/style_transfer/samples_stereo_norm'
                    ],
                    tableTitle: 'Table 3: Style Transfer of Music Segments Based on Text Descriptions.',
                    tableId: "table3",
                    headers: ['#', 'GT', 'AudioLDM', 'MusicLDM', 'LDMME (Ours)']
                },
                {
                    descriptionsPath: 'static/samples/gpt/descriptions.txt',
                    audioPaths: [
                        //     'static/samples/gpt/samples_audioldm',
                        //     'static/samples/gpt/samples_musicldm',
                        //     'static/samples/gpt/samples_stereo'
                        'static/samples/gpt/samples_audioldm_norm',
                        'static/samples/gpt/samples_musicldm_norm',
                        'static/samples/gpt/samples_stereo_norm'
                    ],
                    tableTitle: 'Table 4: Music Generation Based on Text Descriptions Generated from GPT.',
                    tableId: "table4",
                    headers: ['#', 'AudioLDM', 'MusicLDM', 'LDMME (Ours)']
                },
                // {
                //     descriptionsPath: 'static/samples/gpt/descriptions.txt',
                //     audioPaths: [
                //         'static/samples/gpt/samples_audioldm_norm',
                //         'static/samples/gpt/samples_musicldm_norm',
                //         'static/samples/gpt/samples_stereo_norm'
                //     ],
                //     tableTitle: 'Table 5: Music Generation Based on Text Descriptions Generated from GPT norm.',
                //     tableId: "table5",
                //     headers: ['#', 'AudioLDM', 'MusicLDM', 'LDMME (Ours)']
                // }
                // Add more configurations as needed
            ];

            for (const config of tablesConfig) {
                const descriptions = await loadDescriptions(config.descriptionsPath);
                createTable('tablesContainer', descriptions, config.audioPaths, config.tableTitle, config.headers, config.tableId);
            }
        }

        // Load tables when the page loads
        window.onload = initializeTables;
    </script>

    <script>
        // window.onload = function () {
        //     function generateTOC() {
        //         const toc = document.getElementById('toc');
        //         const headings = document.querySelectorAll('h1, h2, h3, h4, h5, h6');
        //         const tocList = document.createElement('ul');
        //         let previousLevel = 1;
        //         let currentList = tocList;
        //         let listStack = [tocList];

        //         headings.forEach(heading => {
        //             const level = parseInt(heading.tagName.substring(1));
        //             const listItem = document.createElement('li');
        //             const link = document.createElement('a');
        //             link.href = `#${heading.id || (heading.id = heading.textContent.replace(/\s+/g, '-').toLowerCase())}`;
        //             link.textContent = heading.textContent;
        //             listItem.appendChild(link);

        //             if (level > previousLevel) {
        //                 const newList = document.createElement('ul');
        //                 currentList.appendChild(newList);
        //                 listStack.push(newList);
        //             } else if (level < previousLevel) {
        //                 while (level < previousLevel) {
        //                     listStack.pop();
        //                     previousLevel--;
        //                 }
        //             }
        //             currentList = listStack[listStack.length - 1];
        //             currentList.appendChild(listItem);
        //             previousLevel = level;
        //         });

        //         toc.appendChild(tocList);
        //     }

        //     generateTOC();
        //     initializeTables();
        // }
    </script>


</body>

</html>